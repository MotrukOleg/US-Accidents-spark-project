# US Accidents Big Data Analysis with PySpark

## Short Description

This project focuses on large-scale data extraction, preprocessing, transformation, and analysis of the **US Accidents** dataset using PySpark.  
The goal is to implement a full big data pipeline including schema definition, data ingestion, statistical analysis, feature engineering, business-driven transformations, execution plan analysis, and result storage.

The project supports both Docker-based and local PySpark environments.

---

## Contributors

- Ihor Yatsyshyn 
- Oleh Motruk 
- Olesia Yankiv  
- Yana Vyklyuk 
- Yurii Hrabovskii

---

# Responsibility Distribution


## Preparation Stage


**Oleh Motruk** — Repository setup and branch management  
**Yana Vyklyuk** — Dataset description  
**Ihor Yatsyshyn** — .gitignore and README  
**Olesia Yankiv** — Report/Presentation, board creation  
**Yurii Hrabovskii** — Report/Presentation  

**All team members** — Dataset search and selection, PySpark installation  

**Responsible:** Oleh Motruk  

---

## Environment Setup Stage

**Yurii Hrabovskii** — Dockerfile  
**Ihor Yatsyshyn** — Update .gitignore  
**Olesia Yankiv** — main.py  
**Oleh Motruk** — Report/Presentation  
**Yana Vyklyuk** — Report/Presentation  

**All team members** — Local configuration  

**Responsible:** Yurii Hrabovskii  

---

## Data Extraction Stage

**Ihor Yatsyshyn** — Schema definition, Report/Presentation  
**Oleh Motruk** — Schema definition, Report/Presentation  
**Yana Vyklyuk** — Schema definition, Module creation  
**Yurii Hrabovskii** — DataFrame creation  
**Olesia Yankiv** — Validation and correctness testing  

**Responsible:** Yana Vyklyuk  

---

## Data Preprocessing Stage

**Ihor Yatsyshyn** — Numerical feature statistics  
**Oleh Motruk** — Feature informativeness analysis  
**Yana Vyklyuk** — Missing values and duplicate handling  
**Yurii Hrabovskii** — Data type casting and parsing, Report/Presentation  
**Olesia Yankiv** — General dataset statistics, Report /Presentation 

**Responsible:** Ihor Yatsyshyn  

---

# Transformation Stage

**Task:**
- Define 6 business-driven analytical questions  
- At least:
  - 3 queries using filter
  - 2 queries using join
  - 2 queries using groupBy
  - 2 queries using window functions
- Analyze execution plans using explain()

**All team members** — Queries and analysis  
**Yana Vyklyuk** — Report/Presentation  
**Oleh Motruk** — Report/Presentation  

**Responsible:** Olesia Yankiv  

---

# Result Saving Stage

**All team members** — Save results  
**Yurii Hrabovskii** — Update .gitignore  
**Ihor Yatsyshyn** — Report/Presentation  
**Olesia Yankiv** — Report/Presentation  

**Responsible:** Yurii Hrabovskii  

---

---

# Опис

Цей проєкт присвячений видобуванню, попередній обробці, трансформації та аналізу набору даних **US Accidents** з використанням PySpark.  
Метою є реалізація повного пайплайну роботи з великими даними: створення схем, зчитування даних, статистичний аналіз, аналіз інформативності ознак, побудова бізнес-запитів, аналіз планів виконання та збереження результатів.

Проєкт підтримує запуск як через Docker, так і через локальне середовище PySpark.

---

# Учасники

- Ігор Яцишин  
- Олег Мотрук 
- Олеся Янків  
- Юрій Грабовський  
- Яна Виклюк  

---

# Розподіл обов’язків

---

# Підготовчий етап

**Олег Мторук** — репозиторій і гілки  
**Яна Виклюк** — опис даних  
**Ігор Яцишин** — .gitignore, README  
**Олеся Янків** — звіт/презентація, створення борди  
**Юрій Грабовський** — звіт/презентація  

**ВСІ** — пошук і вибір даних, встановлення PySpark  

**Відповідальний:** Олег Мотрук  

---

# Етап налаштування

**Юрій Грабовський** — Dockerfile  
**Ігор Яцишин** — оновити .gitignore  
**Олеся Янків** — main.py  
**Олег Мторук** — звіт/презентація  
**Яна Виклюк** — звіт/презентація  

**ВСІ** — локальна конфігурація  

**Відповідальний:** Юрій Грабовський  

---

# Етап видобування

**Ігор Яцишин** — створення схем, звіт/презентація  
**Олег Мторук** — створення схем, звіт/презентація  
**Яна Виклюк** — створення схем, модуль  
**Юрій Грабовський** — створення DataFrame  
**Олеся Янків** — перевірка коректності  

**Відповідальний:** Яна Виклюк  

---

# Етап попередньої обробки 

**Ігор Яцишин** — статистика числових ознак  
**Олег Мторук** — аналіз інформативності ознак  
**Яна Виклюк** — аналіз пропущених значень та дублікатів  
**Юрій Грабовський** — приведення типів та парсинг, звіт  
**Олеся Янків** — загальна статистична інформація, звіт  

**Відповідальний:** Ігор Яцишин  

---

# Етап трансформації

**Завдання:**
- Сформувати 6 бізнес-питань  
- Мінімальні вимоги:
  - не менше 3 запитів з filter
  - не менше 2 з join
  - не менше 2 з groupBy
  - не менше 2 з window functions
- Проаналізувати плани виконання через explain()

**Яна Виклюк** — звіт/презентація  
**Олег Мторук** — звіт/презентація  

**ВСІ** — запити та аналіз  

**Відповідальний:** Олеся Янків  

---

# Етап запису результатів

**Юрій Грабовський** — оновити .gitignore  
**Ігор Яцишин** — звіт/презентація  
**Олеся Янків** — звіт/презентація  

**ВСІ** — збереження результатів  

**Відповідальний:** Юрій Грабовський  


